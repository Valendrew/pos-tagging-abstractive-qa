{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d1d23b6c",
      "metadata": {
        "id": "d1d23b6c"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Keywords**: Transformers, Question Answering, CoQA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dL4rB4Dv4q5D",
      "metadata": {
        "id": "dL4rB4Dv4q5D"
      },
      "source": [
        "## Task and dataset explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11ada8c8",
      "metadata": {
        "id": "11ada8c8"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47c07553",
      "metadata": {
        "id": "47c07553"
      },
      "source": [
        "### Problem\n",
        "\n",
        "Question Answering (QA) on [CoQA](https://stanfordnlp.github.io/coqa/) dataset: a conversational QA dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4907f8d",
      "metadata": {
        "id": "b4907f8d"
      },
      "source": [
        "### Task\n",
        "\n",
        "Given a question $Q$, a text passage $P$, the task is to generate the answer $A$.<br>\n",
        "$\\rightarrow A$ can be: (i) a free-form text or (ii) unanswerable;\n",
        "\n",
        "**Note**: an question $Q$ can refer to previous dialogue turns. <br>\n",
        "$\\rightarrow$ dialogue history $H$ may be a valuable input to provide the correct answer $A$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b3760b5",
      "metadata": {
        "id": "9b3760b5"
      },
      "source": [
        "### Models\n",
        "\n",
        "We are going to experiment with transformer-based models to define the following models:\n",
        "\n",
        "1.  $A = f_\\theta(Q, P)$\n",
        "\n",
        "2. $A = f_\\theta(Q, P, H)$\n",
        "\n",
        "where $f_\\theta$ is the transformer-based model we have to define with $\\theta$ parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66cfee64",
      "metadata": {
        "id": "66cfee64"
      },
      "source": [
        "### The CoQA dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "996fa650",
      "metadata": {
        "id": "996fa650"
      },
      "source": [
        "<center>\n",
        "    <img src=\"https://drive.google.com/uc?export=view&id=16vrgyfoV42Z2AQX0QY7LHTfrgektEKKh\" width=\"750\"/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6e3e7d0",
      "metadata": {
        "id": "f6e3e7d0"
      },
      "source": [
        "For detailed information about the dataset, feel free to check the original [paper](https://arxiv.org/pdf/1808.07042.pdf).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfb6c37e",
      "metadata": {
        "id": "bfb6c37e"
      },
      "source": [
        "### Rationales\n",
        "\n",
        "Each QA pair is paired with a rationale $R$: it is a text span extracted from the given text passage $P$. <br>\n",
        "$\\rightarrow$ $R$ is not a requested output, but it can be used as an additional information at training time!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daa786e2",
      "metadata": {
        "id": "daa786e2"
      },
      "source": [
        "### Dataset Statistics\n",
        "\n",
        "* **127k** QA pairs.\n",
        "* **8k** conversations.\n",
        "* **7** diverse domains: Children's Stories, Literature, Mid/High School Exams, News, Wikipedia, Reddit, Science.\n",
        "* Average conversation length: **15 turns** (i.e., QA pairs).\n",
        "* Almost **half** of CoQA questions refer back to **conversational history**.\n",
        "* Only **train** and **validation** sets are available."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26d68b7",
      "metadata": {
        "id": "d26d68b7"
      },
      "source": [
        "### Dataset snippet\n",
        "\n",
        "The dataset is stored in JSON format. Each dialogue is represented as follows:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"source\": \"mctest\",\n",
        "    \"id\": \"3dr23u6we5exclen4th8uq9rb42tel\",\n",
        "    \"filename\": \"mc160.test.41\",\n",
        "    \"story\": \"Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. \n",
        "    Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. [...]\" % <-- $P$\n",
        "    \"questions\": [\n",
        "        {\n",
        "            \"input_text\": \"What color was Cotton?\",   % <-- $Q_1$\n",
        "            \"turn_id\": 1\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"Where did she live?\",\n",
        "            \"turn_id\": 2\n",
        "        },\n",
        "        [...]\n",
        "    ],\n",
        "    \"answers\": [\n",
        "        {\n",
        "            \"span_start\": 59,   % <-- $R_1$ start index\n",
        "            \"spand_end\": 93,    % <-- $R_1$ end index\n",
        "            \"span_text\": \"a little white kitten named Cotton\",   % <-- $R_1$\n",
        "            \"input_text\" \"white\",   % <-- $A_1$      \n",
        "            \"turn_id\": 1\n",
        "        },\n",
        "        [...]\n",
        "    ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c7558c",
      "metadata": {
        "id": "72c7558c"
      },
      "source": [
        "### Simplifications\n",
        "\n",
        "Each dialogue also contains an additional field ```additional_answers```. For simplicity, we **ignore** this field and only consider one groundtruth answer $A$ and text rationale $R$.\n",
        "\n",
        "CoQA only contains 1.3% of unanswerable questions. For simplicity, we **ignore** those QA pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b532042",
      "metadata": {
        "id": "7b532042"
      },
      "source": [
        "## List of tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01cdad7",
      "metadata": {
        "id": "e01cdad7"
      },
      "source": [
        "### [Task 1] Remove unaswerable QA pairs \n",
        "\n",
        "Write your own script to remove unaswerable QA pairs from both train and validation sets. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f57334e0",
      "metadata": {
        "id": "f57334e0"
      },
      "source": [
        "### [Task 2] Train, Validation and Test splits\n",
        "\n",
        "CoQA only provides a train and validation set since the test set is hidden for evaluation purposes.\n",
        "\n",
        "We'll consider the provided validation set as a test set. <br>\n",
        "$\\rightarrow$ Write your own script to:\n",
        "* Split the train data in train and validation splits (80% train and 20% val)\n",
        "* Perform splits such that a dialogue appears in one split only! (i.e., split at dialogue level)\n",
        "* Perform splitting using the following seed for reproducibility: 42\n",
        "\n",
        "#### Reproducibility Memo\n",
        "\n",
        "Check back tutorial 2 on how to fix a specific random seed for reproducibility!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "230a21de",
      "metadata": {
        "id": "230a21de"
      },
      "source": [
        "### [Task 3] Model definition\n",
        "\n",
        "Write your own script to define the following transformer-based models from [huggingface](https://HuggingFace.co/).\n",
        "\n",
        "* [M1] DistilRoBERTa (distilberta-base)\n",
        "* [M2] BERTTiny (bert-tiny)\n",
        "\n",
        "**Note**: Remember to install the ```transformers``` python package!\n",
        "\n",
        "**Note**: We consider small transformer models for computational reasons!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1e83f28",
      "metadata": {
        "id": "f1e83f28"
      },
      "source": [
        "### [Task 4] Question generation with text passage $P$ and question $Q$\n",
        "\n",
        "We want to define $f_\\theta(P, Q)$. \n",
        "\n",
        "Write your own script to implement $f_\\theta$ for each model: M1 and M2.\n",
        "\n",
        "#### Formulation\n",
        "\n",
        "Consider a dialogue on text passage $P$. \n",
        "\n",
        "For each question $Q_i$ at dialogue turn $i$, your model should take $P$ and $Q_i$ and generate $A_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7311ba86",
      "metadata": {
        "id": "7311ba86"
      },
      "source": [
        "### [Task 5] Question generation with text passage $P$, question $Q$ and dialogue history $H$\n",
        "\n",
        "We want to define $f_\\theta(P, Q, H)$. Write your own script to implement $f_\\theta$ for each model: M1 and M2.\n",
        "\n",
        "#### Formulation\n",
        "\n",
        "Consider a dialogue on text passage $P$. \n",
        "\n",
        "For each question $Q_i$ at dialogue turn $i$, your model should take $P$, $Q_i$, and $H = \\{ Q_0, A_0, \\dots, Q_{i-1}, A_{i-1} \\}$ to generate $A_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5ac768c",
      "metadata": {
        "id": "b5ac768c"
      },
      "source": [
        "### [Task 6] Train and evaluate $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$\n",
        "\n",
        "Write your own script to train and evaluate your $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$ models.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "* Perform multiple train/evaluation seed runs: [42, 2022, 1337].$^1$\n",
        "* Evaluate your models with the following metrics: SQUAD F1-score.$^2$\n",
        "* Fine-tune each transformer-based models for **3 epochs**.\n",
        "* Report evaluation SQUAD F1-score computed on the validation and test sets.\n",
        "\n",
        "$^1$ Remember what we said about code reproducibility in Tutorial 2!\n",
        "\n",
        "$^2$ You can use ```allennlp``` python package for a quick implementation of SQUAD F1-score: ```from allennlp_models.rc.tools import squad```. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92c7e98f",
      "metadata": {
        "id": "92c7e98f"
      },
      "source": [
        "### [Task 7] Error Analysis\n",
        "\n",
        "Perform a simple and short error analysis as follows:\n",
        "* Group dialogues by ```source``` and report the worst 5 model errors for each source (w.r.t. SQUAD F1-score).\n",
        "* Inspect observed results and try to provide some comments (e.g., do the models make errors when faced with a particular question type?)$^1$\n",
        "\n",
        "$^1$ Check the [paper](https://arxiv.org/pdf/1808.07042.pdf) for some valuable information about question/answer types (e.g., Table 6, Table 8) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6643e14",
      "metadata": {
        "id": "f6643e14"
      },
      "source": [
        "## Dataset Download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6cZJRT4tU1ms",
      "metadata": {
        "id": "6cZJRT4tU1ms"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import json\n",
        "import typing\n",
        "import urllib.request\n",
        "import random\n",
        "\n",
        "import keras.callbacks\n",
        "import keras.layers\n",
        "import keras.losses\n",
        "import keras.optimizers\n",
        "import keras.regularizers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "358bac70",
      "metadata": {
        "id": "358bac70"
      },
      "outputs": [],
      "source": [
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "        \n",
        "def download_url(url, output_path):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
        "\n",
        "def download_data(data_path, url_path, suffix):    \n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "        \n",
        "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
        "        download_url(url=url_path, output_path=data_path)\n",
        "        print(\"Download completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5f6ab3ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f6ab3ff",
        "outputId": "4369a01b-bf21-41b5-ffec-558f724cc424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading CoQA train data split... (it may take a while)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "coqa-train-v1.0.json: 49.0MB [00:06, 7.92MB/s]                            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download completed!\n",
            "Downloading CoQA test data split... (it may take a while)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "coqa-dev-v1.0.json: 9.09MB [00:01, 6.86MB/s]                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download completed!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train data\n",
        "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
        "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
        "\n",
        "# Test data\n",
        "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
        "download_data(data_path='coqa', url_path=test_url, suffix='test')  # <-- Why test? See next slides for an answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bddb1b09",
      "metadata": {
        "id": "bddb1b09"
      },
      "source": [
        "## Create the dataframe, inspect the data and do some preprocessing [TASK 1, TASK 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "89e33d91",
      "metadata": {
        "id": "89e33d91"
      },
      "outputs": [],
      "source": [
        "train_path = './coqa/train.json'\n",
        "test_path = './coqa/test.json'\n",
        "\n",
        "with open(train_path, 'r') as f:\n",
        "    train_json = json.load(f)\n",
        "\n",
        "with open(test_path, 'r') as f:\n",
        "    test_json = json.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JARrWA8lmPO_",
      "metadata": {
        "id": "JARrWA8lmPO_"
      },
      "source": [
        "Given that the dataset is provided as a json file we decided to unpack it with a first rough operation creating a column of a DataFrame for each key of the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9a10f052",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9a10f052",
        "outputId": "cab8e308-20f7-41c9-dbb1-d368e8ccb6e8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e2a598b1-46f0-4593-a035-3e06e7483e3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>id</th>\n",
              "      <th>filename</th>\n",
              "      <th>story</th>\n",
              "      <th>questions</th>\n",
              "      <th>answers</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wikipedia</td>\n",
              "      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n",
              "      <td>Vatican_Library.txt</td>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>[{'input_text': 'When was the Vat formally ope...</td>\n",
              "      <td>[{'span_start': 151, 'span_end': 179, 'span_te...</td>\n",
              "      <td>Vatican_Library.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3wj1oxy92agboo5nlq4r7bndc3t8a8</td>\n",
              "      <td>cnn_fe05c61a7e48461f7883cdec387567029614f07b.s...</td>\n",
              "      <td>New York (CNN) -- More than 80 Michael Jackson...</td>\n",
              "      <td>[{'input_text': 'Where was the Auction held?',...</td>\n",
              "      <td>[{'span_start': 243, 'span_end': 284, 'span_te...</td>\n",
              "      <td>cnn_fe05c61a7e48461f7883cdec387567029614f07b.s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3bdcf01ogxu7zdn9vlrbf2rqzwplyf</td>\n",
              "      <td>data/gutenberg/txt/Zane Grey___Riders of the P...</td>\n",
              "      <td>CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"...</td>\n",
              "      <td>[{'input_text': 'What did Venters call Lassite...</td>\n",
              "      <td>[{'span_start': 841, 'span_end': 880, 'span_te...</td>\n",
              "      <td>data/gutenberg/txt/Zane Grey___Riders of the P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cnn</td>\n",
              "      <td>3ewijtffvo7wwchw6rtyaf7mfwte0p</td>\n",
              "      <td>cnn_0c518067e0df811501e46b2e1cd1ce511f1645b7.s...</td>\n",
              "      <td>(CNN) -- The longest-running holiday special s...</td>\n",
              "      <td>[{'input_text': 'Who is Rudolph's father?', 't...</td>\n",
              "      <td>[{'span_start': 500, 'span_end': 557, 'span_te...</td>\n",
              "      <td>cnn_0c518067e0df811501e46b2e1cd1ce511f1645b7.s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gutenberg</td>\n",
              "      <td>3urfvvm165iantk80llvkwwbjs7uzh</td>\n",
              "      <td>data/gutenberg/txt/Rafael Sabatini___Love-at-A...</td>\n",
              "      <td>CHAPTER XXIV. THE INTERRUPTED MASS \\n\\nThe mor...</td>\n",
              "      <td>[{'input_text': 'Who arrived at the church?', ...</td>\n",
              "      <td>[{'span_start': 254, 'span_end': 297, 'span_te...</td>\n",
              "      <td>data/gutenberg/txt/Rafael Sabatini___Love-at-A...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2a598b1-46f0-4593-a035-3e06e7483e3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2a598b1-46f0-4593-a035-3e06e7483e3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2a598b1-46f0-4593-a035-3e06e7483e3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      source                              id  \\\n",
              "0  wikipedia  3zotghdk5ibi9cex97fepx7jetpso7   \n",
              "1        cnn  3wj1oxy92agboo5nlq4r7bndc3t8a8   \n",
              "2  gutenberg  3bdcf01ogxu7zdn9vlrbf2rqzwplyf   \n",
              "3        cnn  3ewijtffvo7wwchw6rtyaf7mfwte0p   \n",
              "4  gutenberg  3urfvvm165iantk80llvkwwbjs7uzh   \n",
              "\n",
              "                                            filename  \\\n",
              "0                                Vatican_Library.txt   \n",
              "1  cnn_fe05c61a7e48461f7883cdec387567029614f07b.s...   \n",
              "2  data/gutenberg/txt/Zane Grey___Riders of the P...   \n",
              "3  cnn_0c518067e0df811501e46b2e1cd1ce511f1645b7.s...   \n",
              "4  data/gutenberg/txt/Rafael Sabatini___Love-at-A...   \n",
              "\n",
              "                                               story  \\\n",
              "0  The Vatican Apostolic Library (), more commonl...   \n",
              "1  New York (CNN) -- More than 80 Michael Jackson...   \n",
              "2  CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"...   \n",
              "3  (CNN) -- The longest-running holiday special s...   \n",
              "4  CHAPTER XXIV. THE INTERRUPTED MASS \\n\\nThe mor...   \n",
              "\n",
              "                                           questions  \\\n",
              "0  [{'input_text': 'When was the Vat formally ope...   \n",
              "1  [{'input_text': 'Where was the Auction held?',...   \n",
              "2  [{'input_text': 'What did Venters call Lassite...   \n",
              "3  [{'input_text': 'Who is Rudolph's father?', 't...   \n",
              "4  [{'input_text': 'Who arrived at the church?', ...   \n",
              "\n",
              "                                             answers  \\\n",
              "0  [{'span_start': 151, 'span_end': 179, 'span_te...   \n",
              "1  [{'span_start': 243, 'span_end': 284, 'span_te...   \n",
              "2  [{'span_start': 841, 'span_end': 880, 'span_te...   \n",
              "3  [{'span_start': 500, 'span_end': 557, 'span_te...   \n",
              "4  [{'span_start': 254, 'span_end': 297, 'span_te...   \n",
              "\n",
              "                                                name  \n",
              "0                                Vatican_Library.txt  \n",
              "1  cnn_fe05c61a7e48461f7883cdec387567029614f07b.s...  \n",
              "2  data/gutenberg/txt/Zane Grey___Riders of the P...  \n",
              "3  cnn_0c518067e0df811501e46b2e1cd1ce511f1645b7.s...  \n",
              "4  data/gutenberg/txt/Rafael Sabatini___Love-at-A...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train = pd.json_normalize(train_json['data'])\n",
        "df_test = pd.json_normalize(test_json['data'])\n",
        "# We need a different DataFrame that consider also the 'source' for the final evaluation\n",
        "df_final_evaluation = df_test.copy()\n",
        "\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9tsuAVrvmi9h",
      "metadata": {
        "id": "9tsuAVrvmi9h"
      },
      "source": [
        "As we can see, the sources for the dialogues are only five and they provide almost the same number of dialogues, apart from 'mctest'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eEdxQ5ALm4tu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEdxQ5ALm4tu",
        "outputId": "49ceecdc-197c-49c1-96f5-cd6cdb7b2a8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "race         1711\n",
              "cnn          1702\n",
              "wikipedia    1621\n",
              "gutenberg    1615\n",
              "mctest        550\n",
              "Name: source, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train['source'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3qaItcj1neoE",
      "metadata": {
        "id": "3qaItcj1neoE"
      },
      "source": [
        "We can also explore the number of dialogues and the average lenght of question-answer pairs for the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "H4HKsw8jnlZm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4HKsw8jnlZm",
        "outputId": "cdc181b0-67f9-4b54-82b5-893d2c46be38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- The training set contains 7199 dialogues.\n",
            "\n",
            "- On average we have 15.09 question-answer pairs for each dialogue.\n"
          ]
        }
      ],
      "source": [
        "print(f\"- The training set contains {len(df_train)} dialogues.\\n\")\n",
        "print(f\"- On average we have {round(df_train['questions'].apply(lambda x: len(x)).mean(),2)}\"\n",
        "      f\" question-answer pairs for each dialogue.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "730969bf",
      "metadata": {
        "id": "730969bf"
      },
      "source": [
        "In the dataset above we have some features that are not useful for our task therefore we decided to remove them and we remained with the story for each dialogue, the questions and the relative answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "af8558f0",
      "metadata": {
        "id": "af8558f0"
      },
      "outputs": [],
      "source": [
        "features_to_remove = ['source', 'id', 'filename', 'name']\n",
        "features_to_remove_test = features_to_remove + [f'additional_answers.{i}' for i in range(3)]\n",
        "\n",
        "df_train.drop(features_to_remove, axis=1, inplace=True)\n",
        "df_test.drop(features_to_remove_test, axis=1, inplace=True)\n",
        "df_final_evaluation.drop(features_to_remove_test[1:], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6bce7aec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6bce7aec",
        "outputId": "17cd09df-404f-4255-9aa3-64897d931a14"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-268174d2-cd73-45d0-98c6-6e7994c81e47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>questions</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
              "      <td>[{'input_text': 'When was the Vat formally ope...</td>\n",
              "      <td>[{'span_start': 151, 'span_end': 179, 'span_te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>New York (CNN) -- More than 80 Michael Jackson...</td>\n",
              "      <td>[{'input_text': 'Where was the Auction held?',...</td>\n",
              "      <td>[{'span_start': 243, 'span_end': 284, 'span_te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"...</td>\n",
              "      <td>[{'input_text': 'What did Venters call Lassite...</td>\n",
              "      <td>[{'span_start': 841, 'span_end': 880, 'span_te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(CNN) -- The longest-running holiday special s...</td>\n",
              "      <td>[{'input_text': 'Who is Rudolph's father?', 't...</td>\n",
              "      <td>[{'span_start': 500, 'span_end': 557, 'span_te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CHAPTER XXIV. THE INTERRUPTED MASS \\n\\nThe mor...</td>\n",
              "      <td>[{'input_text': 'Who arrived at the church?', ...</td>\n",
              "      <td>[{'span_start': 254, 'span_end': 297, 'span_te...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-268174d2-cd73-45d0-98c6-6e7994c81e47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-268174d2-cd73-45d0-98c6-6e7994c81e47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-268174d2-cd73-45d0-98c6-6e7994c81e47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               story  \\\n",
              "0  The Vatican Apostolic Library (), more commonl...   \n",
              "1  New York (CNN) -- More than 80 Michael Jackson...   \n",
              "2  CHAPTER VII. THE DAUGHTER OF WITHERSTEEN \\n\\n\"...   \n",
              "3  (CNN) -- The longest-running holiday special s...   \n",
              "4  CHAPTER XXIV. THE INTERRUPTED MASS \\n\\nThe mor...   \n",
              "\n",
              "                                           questions  \\\n",
              "0  [{'input_text': 'When was the Vat formally ope...   \n",
              "1  [{'input_text': 'Where was the Auction held?',...   \n",
              "2  [{'input_text': 'What did Venters call Lassite...   \n",
              "3  [{'input_text': 'Who is Rudolph's father?', 't...   \n",
              "4  [{'input_text': 'Who arrived at the church?', ...   \n",
              "\n",
              "                                             answers  \n",
              "0  [{'span_start': 151, 'span_end': 179, 'span_te...  \n",
              "1  [{'span_start': 243, 'span_end': 284, 'span_te...  \n",
              "2  [{'span_start': 841, 'span_end': 880, 'span_te...  \n",
              "3  [{'span_start': 500, 'span_end': 557, 'span_te...  \n",
              "4  [{'span_start': 254, 'span_end': 297, 'span_te...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd3b662",
      "metadata": {
        "id": "9cd3b662"
      },
      "source": [
        "At this point we can split the dataset in training and validation as stated in the assignment, we do that operation here because the questions are still grouped in dialogues therefore we put all the questions from one dialogue either in validation or in the training set, as specified in ***Task 2***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "IGstSaVRpVLh",
      "metadata": {
        "id": "IGstSaVRpVLh"
      },
      "outputs": [],
      "source": [
        "seeds = [42, 2022, 1337]\n",
        "\n",
        "def set_reproducibility(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "AK-cFhGWwTvv",
      "metadata": {
        "id": "AK-cFhGWwTvv"
      },
      "outputs": [],
      "source": [
        "def split_train_validation(df: pd.DataFrame, val_size: float=0.2) -> typing.Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    '''\n",
        "        It takes as input a dataframe and it splits it in 2 dataframes considering the \n",
        "        'val_size' parameter. \n",
        "    '''\n",
        "\n",
        "    validation = df.sample(frac=val_size) \n",
        "    training = df.drop(validation.index)\n",
        "\n",
        "    assert training is not None, \"The dataframe is None\"\n",
        "    # It shuffles the training set dialogues\n",
        "    training = training.sample(frac=1)\n",
        "\n",
        "    return training.reset_index(drop=True), validation.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6u_lmheKo0vK",
      "metadata": {
        "id": "6u_lmheKo0vK"
      },
      "source": [
        "Before splitting the training data in training and validation set we set the seed 42 for the reproducibility as requested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a63f6ff1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a63f6ff1",
        "outputId": "521a265d-6369-4922-e40b-52566becc197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The dimensions of the datasets are:\n",
            "Training dataset -> (5759, 3) - Validation dataset -> (1440, 3)\n"
          ]
        }
      ],
      "source": [
        "set_reproducibility(seeds[0])\n",
        "df_train, df_val = split_train_validation(df_train, 0.2)\n",
        "print(f\"The dimensions of the datasets are:\\n\"\n",
        "     f\"Training dataset -> {df_train.shape} - Validation dataset -> {df_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JD_WpQG4pOGS",
      "metadata": {
        "id": "JD_WpQG4pOGS"
      },
      "source": [
        "We still have to unpack the array of questions and answers in order to create a row for each single element and we need to remove the unanswerable questions from the data.\n",
        "- The first function is a helper that allows us to create different rows of a DataFrame from a list of questions. \n",
        "- The second one is used to remove the unanswerable questions from the data, a question is ***unanswerable*** if the 'span_start' feature is -1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "be70ee1a",
      "metadata": {
        "id": "be70ee1a"
      },
      "outputs": [],
      "source": [
        "def expand_lists(df: pd.DataFrame, to_drop: typing.List[str]=[],\n",
        "                 for_eval: bool=False) -> pd.DataFrame:\n",
        "    '''\n",
        "        Given a Pandas dataframe it returns a new dataframe with the expansion of\n",
        "        the questions and the answers and removing the desired columns.\n",
        "    '''\n",
        "    if for_eval:\n",
        "        # Add also the 'source' feature\n",
        "        assert 'source' in df.columns, \"The DataFrame has not the 'source' feature\"\n",
        "        questions = [{'source': df.source[i], 'story':df.story[i], **quest} \n",
        "                        for i, lis in enumerate(df.questions) for quest in lis]\n",
        "    else:\n",
        "        # Create a dictionary with the story and the other features for each question\n",
        "        questions = [{'story':df.story[i], **quest} for i, lis in enumerate(df.questions) for quest in lis]\n",
        "\n",
        "    answers = [ans for lis in df.answers for ans in lis]\n",
        "\n",
        "    # Create a DataFrame from the previous dictionaries and remove useless features\n",
        "    X = pd.DataFrame.from_dict(questions, orient=\"columns\").drop(to_drop, axis=1)\n",
        "    y = pd.DataFrame.from_dict(answers, orient=\"columns\").drop(to_drop, axis=1)\n",
        "\n",
        "    assert X is not None and y is not None, \"The dataframe is None\"\n",
        "    \n",
        "    X.rename(columns={'input_text':'questions'}, inplace=True)\n",
        "    y.rename(columns={'input_text':'answers'}, inplace=True)\n",
        "    \n",
        "    return pd.concat([X, y], axis=1)\n",
        "\n",
        "\n",
        "def remove_unanswerable(df: pd.DataFrame, verbose:bool=True) -> pd.DataFrame:\n",
        "    '''\n",
        "        It removes the unanswerable questions from the passed DataFrame\n",
        "        removing the elements with 'span_start' < 0\n",
        "    '''\n",
        "\n",
        "    target = 'span_start'\n",
        "    if target not in df.columns:\n",
        "        print(\n",
        "            \"WARNING: the DataFrame doesn't have the 'span_start' column, the\"\n",
        "            \" function will return the DataFrame without changes.\"\n",
        "            )\n",
        "        return df\n",
        "        \n",
        "    uns_rows = df[target] > 0\n",
        "    if verbose:\n",
        "        print(\n",
        "            f\"{len(df) - len(df[uns_rows])} unanswerable questions has been removed\"\n",
        "             \" from the DataFrame.\"\n",
        "            )\n",
        "    return df[uns_rows].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u1ufKLEVpgE8",
      "metadata": {
        "id": "u1ufKLEVpgE8"
      },
      "source": [
        "In order to make the code as ordered as possible we implemented a unique function for the preprocessing that we need to apply on the different DataFrames. It simply calls the 2 previous functions on the passed DataFrame, it removes the features about the spans and it returns the new DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "VEPGym-Q-d6K",
      "metadata": {
        "id": "VEPGym-Q-d6K"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing(df: pd.DataFrame, to_drop: typing.List[str]=[],\n",
        "                       for_eval: bool=False) -> pd.DataFrame:\n",
        "    '''\n",
        "        This function creates a DataFrame with a column for the 'story', one\n",
        "        for the 'questions' and another for the 'answers' and it removes the\n",
        "        unanswerable questions.\n",
        "        Parameters:\n",
        "            - df : pd.DataFrame\n",
        "                The structure on which performing the operations.\n",
        "            - to_drop: list[str]\n",
        "                The names of the columns to drop.\n",
        "            - for_eval: bool\n",
        "                If the expand_lists function has to be run in evaluation mode,\n",
        "                so we preserve the 'source' feature.\n",
        "                \n",
        "        Returns:\n",
        "            pandas.Dataframe\n",
        "                The modified dataframe.\n",
        "    '''\n",
        "    new_df = expand_lists(df, to_drop, for_eval)\n",
        "    new_df = remove_unanswerable(new_df)\n",
        "\n",
        "    span_feat = ['span_start', 'span_end', 'span_text']\n",
        "    new_df.drop(span_feat, axis=1, inplace=True)\n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6e6f82ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e6f82ee",
        "outputId": "920ab3b9-0687-4e5b-84cc-e5428f2d3af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3490 unanswerable questions has been removed from the DataFrame.\n",
            "900 unanswerable questions has been removed from the DataFrame.\n",
            "288 unanswerable questions has been removed from the DataFrame.\n",
            "288 unanswerable questions has been removed from the DataFrame.\n",
            "\n",
            "The training set has 83408 samples, the validation set has 20849 samplesand the test set has 7695 samples.\n"
          ]
        }
      ],
      "source": [
        "df_train = data_preprocessing(df_train, to_drop=['turn_id', 'bad_turn'])\n",
        "df_val = data_preprocessing(df_val, to_drop=['turn_id', 'bad_turn'])\n",
        "df_test = data_preprocessing(df_test, to_drop=['turn_id'])\n",
        "df_final_evaluation = data_preprocessing(df_final_evaluation, to_drop=['turn_id'], for_eval=True)\n",
        "\n",
        "print(f\"\\nThe training set has {len(df_train)} samples, the validation set has {len(df_val)} samples\"\n",
        "      f\"and the test set has {len(df_test)} samples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aWpccDgqEVbs",
      "metadata": {
        "id": "aWpccDgqEVbs"
      },
      "source": [
        "At this moment the DataFrame looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ahNnLLGVEU6p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ahNnLLGVEU6p",
        "outputId": "d8f4d768-16a2-42f7-d2a5-d25b0fb49b23"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3f388961-235e-422f-acd5-476a517b34e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>questions</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CHAPTER III. \\n\\nTHE NIGHT-WATCHERS. \\n\\nWhat ...</td>\n",
              "      <td>What color is the top of the church?</td>\n",
              "      <td>green</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CHAPTER III. \\n\\nTHE NIGHT-WATCHERS. \\n\\nWhat ...</td>\n",
              "      <td>Is it short?</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CHAPTER III. \\n\\nTHE NIGHT-WATCHERS. \\n\\nWhat ...</td>\n",
              "      <td>Who behaves better?</td>\n",
              "      <td>the back of the Tenements</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CHAPTER III. \\n\\nTHE NIGHT-WATCHERS. \\n\\nWhat ...</td>\n",
              "      <td>Why?</td>\n",
              "      <td>Every back window in the Tenements has a glint...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CHAPTER III. \\n\\nTHE NIGHT-WATCHERS. \\n\\nWhat ...</td>\n",
              "      <td>Who killed himself?</td>\n",
              "      <td>Beattie</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f388961-235e-422f-acd5-476a517b34e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f388961-235e-422f-acd5-476a517b34e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f388961-235e-422f-acd5-476a517b34e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               story  \\\n",
              "0  CHAPTER III. \\n\\nTHE NIGHT-WATCHERS. \\n\\nWhat ...   \n",
              "1  CHAPTER III. \\n\\nTHE NIGHT-WATCHERS. \\n\\nWhat ...   \n",
              "2  CHAPTER III. \\n\\nTHE NIGHT-WATCHERS. \\n\\nWhat ...   \n",
              "3  CHAPTER III. \\n\\nTHE NIGHT-WATCHERS. \\n\\nWhat ...   \n",
              "4  CHAPTER III. \\n\\nTHE NIGHT-WATCHERS. \\n\\nWhat ...   \n",
              "\n",
              "                              questions  \\\n",
              "0  What color is the top of the church?   \n",
              "1                          Is it short?   \n",
              "2                   Who behaves better?   \n",
              "3                                  Why?   \n",
              "4                   Who killed himself?   \n",
              "\n",
              "                                             answers  \n",
              "0                                              green  \n",
              "1                                                 No  \n",
              "2                          the back of the Tenements  \n",
              "3  Every back window in the Tenements has a glint...  \n",
              "4                                            Beattie  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yDRKGGZhhDYh",
      "metadata": {
        "id": "yDRKGGZhhDYh"
      },
      "source": [
        "## Model implementation and data tokenization [TASK 3, TASK 4, TASK 5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RIIrgApsE0LM",
      "metadata": {
        "id": "RIIrgApsE0LM"
      },
      "source": [
        "In order to train the encoder-decoder architecture we have to tokenize the samples such that we can give numbers as input to the network.\n",
        "\n",
        "In our case we will use the tokenizer of the relative pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ih3U7eVgy2",
      "metadata": {
        "id": "86ih3U7eVgy2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "!pip install datasets -q\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "icicV7K0sEzm",
      "metadata": {
        "id": "icicV7K0sEzm"
      },
      "source": [
        "### Load the pre-trained model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "p-v8bD-1sP7i",
      "metadata": {
        "id": "p-v8bD-1sP7i"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Union, List, Tuple, Dict\n",
        "\n",
        "from datasets.arrow_dataset import Dataset\n",
        "from transformers import TFEncoderDecoderModel, AutoTokenizer\n",
        "\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "5OH0olNXsTF2",
      "metadata": {
        "id": "5OH0olNXsTF2"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "drive_weights_path = \"/content/drive/MyDrive/weights_NLP_assignments/\"\n",
        "drive_tokenized_path = \"/content/drive/MyDrive/weights_NLP_assignments/tok_data/\"\n",
        "\n",
        "bertiny_name = \"prajjwal1/bert-tiny\"\n",
        "roberta_name = \"distilroberta-base\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mrMByUePtCjm",
      "metadata": {
        "id": "mrMByUePtCjm"
      },
      "source": [
        "We implemented a function to import the tokenizer and the model from a pre-saved model or directly from Huggingface if no path is passed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "usyK7nEHsZBK",
      "metadata": {
        "id": "usyK7nEHsZBK"
      },
      "outputs": [],
      "source": [
        "def check_pretrained_path(path: str) -> bool:\n",
        "    '''\n",
        "        It checks if the config.json file exists in the current path.\n",
        "    '''\n",
        "    config_file = os.path.join(path, 'config.json')\n",
        "    return os.path.exists(config_file)\n",
        "\n",
        "\n",
        "def import_model_tokenizer(name: str=\"distilroberta-base\", load_weights: Optional[str]=None)->tuple:\n",
        "    '''\n",
        "        It imports the tokenizer and the model specified from the 'name' parameter.\n",
        "        If a path in 'load_weigths' is specified then it imports the model from\n",
        "        a pre-saved file.\n",
        "        Parameters:\n",
        "            - name: str\n",
        "                The name of the model we want to import, it must exist in the\n",
        "                Huggingface database.\n",
        "            - load_weights: str | None\n",
        "                The path to a directory that contains the config file and the \n",
        "                .h5 file for loading a pre-saved model.\n",
        "        \n",
        "        Returns:\n",
        "            - tuple\n",
        "                It returns the desired tokenizer and the imported model.\n",
        "    '''\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    enc_from_pt = False\n",
        "\n",
        "    # If a path has been passed\n",
        "    if load_weights != None:\n",
        "        mod_name = name.split('/')[-1]\n",
        "        full_path = os.path.join(drive_weights_path, mod_name, load_weights)\n",
        "        if check_pretrained_path(full_path):\n",
        "            model = TFEncoderDecoderModel.from_pretrained(full_path)\n",
        "        else:\n",
        "            print(\"ERROR: Check if a config.json file exists in the given path.\")\n",
        "            return None, None\n",
        "    else:\n",
        "        if name != \"distilroberta-base\":\n",
        "            enc_from_pt = True\n",
        "\n",
        "        model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "            name, name, encoder_from_pt=enc_from_pt, decoder_from_pt=enc_from_pt\n",
        "        )\n",
        "        model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "        model.config_eos_token_id = tokenizer.sep_token_id\n",
        "        model.config.vocab_size = model.config.encoder.vocab_size\n",
        "\n",
        "\n",
        "\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "FCQ8VSepsk3v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "b8ddbfb4953c4915915629b4772bc6ba",
            "be3cadb57c6c4d908d611ace714fa9ac",
            "287374fc9a99468b9fcf5e84792c29de",
            "eb637e6eb0334cceb8cc12fa2c99cd37",
            "808fe8190ddb46758922fe04b16ced4e",
            "280ac3d6edd541ac9a10af585918baf1",
            "6e08563dbfca4c798e5a717c29664d13",
            "4fb6e7edd7b74a918e051e7a0b8150a5",
            "a7bc0fbac0f94a1a802a6270c9428c53",
            "09d2d44282ab4393a1194b3b2e22d0aa",
            "496f22c495e641d39b6c1de47a92ccf7",
            "0e68baefb0ea44be891c8dc781dc8ee6",
            "a5c72d8bac0146b78146706d434af159",
            "83abbfc0af0e48f299320794de8efd6e",
            "6d127a372be54e81af2b963bba3f780b",
            "15a7fa9330594048b97a81c3c0d8d03f",
            "e7ec7aaa3490495aab850ae847c66027",
            "4273cbab512b4e1cab8f8d671b0c8982",
            "571a2f1dcc1449058216e47ec26ae1a0",
            "d8ee2155725e4d71b19fa34e3c2dd81c",
            "4830313952d04817afcc32ee388d3a2b",
            "2c08a3707b3148858d407122ab78ee6c",
            "57397b6e3f204e15bc7b51a646a04eda",
            "73f009ed1d2e48babeb14d86ee63f4a0",
            "d0914d33146646ee8405229cb920f25e",
            "c8c17855f3194af79a2cb93dee29d940",
            "cae7cd66d398453c980725c505de8b77",
            "1f3ec4fc435f4a58a4073291bed6aaca",
            "dea4cc34b8af4b74946da392a5fc0d39",
            "9bfa7f13133e4968aca726f32c89ef1a",
            "cbff110e03de4dcab47f0dcd1144f442",
            "b5ec124b283c40078b611975b26754d0",
            "7b4eadf9922a40c3830fcb81b5cc359a"
          ]
        },
        "id": "FCQ8VSepsk3v",
        "outputId": "6e710b8c-0438-4733-aa16-8a11b2eb064b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8ddbfb4953c4915915629b4772bc6ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/285 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e68baefb0ea44be891c8dc781dc8ee6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57397b6e3f204e15bc7b51a646a04eda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'bert.embeddings.position_ids', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFBertModel.\n",
            "\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at /tmp/tmpfc5imf4a.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertLMHeadModel: ['cls.predictions.decoder.bias', 'bert.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFBertLMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertLMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFBertLMHeadModel were not initialized from the PyTorch model and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "All model checkpoint layers were used when initializing TFBertLMHeadModel.\n",
            "\n",
            "All the layers of TFBertLMHeadModel were initialized from the model checkpoint at /tmp/tmps6y_qtft.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertLMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer, model = import_model_tokenizer(bertiny_name)#, load_weights='full_history')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C4RB6BoVsmgO",
      "metadata": {
        "id": "C4RB6BoVsmgO"
      },
      "source": [
        "### Tokenization function and history creation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4LQqckhmB9Nt",
      "metadata": {
        "id": "4LQqckhmB9Nt"
      },
      "source": [
        "The following function is needed to create the history for each sample, the history is composed by the 'story' of the question and all the questions and answers from the previous rows.\n",
        "The function has 3 main steps:\n",
        "- we group by 'story' in order to consider the samples belonging to the same dialogue; we create a new column 'history' in the DataFrame where we put for each line the concatenation of answer and question of the successive row. This because the first element of the history for each question is the previous one, so we need to shift by 1 such that we avoid to put the current question and answer in the history as well.\n",
        "- Now, after having grouped by 'story' again, we apply the function *cumulative sum*, it sums the previous rows of the same group to the current one.\n",
        "- As last step we sum the 'story' to the computed history and we return the DataFrame with the new column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fFPB_4uXCBIo",
      "metadata": {
        "id": "fFPB_4uXCBIo"
      },
      "outputs": [],
      "source": [
        "def create_history(df: pd.DataFrame, group: str='story') -> pd.DataFrame:\n",
        "    '''\n",
        "        This function returns the dataframe with a new column 'history' which\n",
        "        contains the sum of the story and all the questions and answers within\n",
        "        the same context, not considering the current sample.\n",
        "        Parameters:\n",
        "            - df: pd.DataFrame\n",
        "                The dataframe from which the function takes the data.\n",
        "            - group: str\n",
        "                The column to consider for the groupby operation.\n",
        "        \n",
        "        Returns:\n",
        "            - pd.DataFrame\n",
        "                The DataFrame with the new column added. \n",
        "    '''\n",
        "    new_df = df.copy()\n",
        "    sum_columns = lambda x: x.questions + ' ' + x.answers + ' '\n",
        "    # If the story has a white space at the end, we don't need to add it\n",
        "    def sum_ctx_history(row):\n",
        "        space = \" \"\n",
        "        if row.story[-1].isspace():\n",
        "            space = \"\"\n",
        "        return row.story + space + row.history\n",
        "\n",
        "    print(\"START: Creating the history column for the dataset...\")\n",
        "    # Returns a new column that contains for each sample the sum of the\n",
        "    # question and the context\n",
        "    new_df['history'] = new_df.groupby(group, axis=0, sort=False) \\\n",
        "                            .shift(fill_value='') \\\n",
        "                            .apply(sum_columns, axis=1)\n",
        "\n",
        "    # Sum the history up to the previous question for each sample\n",
        "    new_df['history'] = new_df.groupby(group, axis=0, sort=False)['history'] \\\n",
        "                            .apply(lambda x: x.cumsum()).str.strip()\n",
        "\n",
        "    new_df['history'] = new_df.apply(sum_ctx_history, axis=1)\n",
        "\n",
        "    print(\"END: The column for the history has been added to the dataset.\")\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XAX8TSBqCCXi",
      "metadata": {
        "id": "XAX8TSBqCCXi"
      },
      "source": [
        "The following functions are used to compute the tokenized answers and the tokenization of each question + the relative context (story / history). The function has a parameter that checks if the history needs to be created, if this is the case the previous function 'create_history' is called and the successive operations are performed with the new dataset.\n",
        "\n",
        "We decided to create batch of samples from the passed dataset in order to avoid the consumption of the RAM, even if we also give the possibiity to save the tokenized data in a .json file in a Drive directory, to make the loading of the data faster.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "P9L8DQnw8QnH",
      "metadata": {
        "id": "P9L8DQnw8QnH"
      },
      "outputs": [],
      "source": [
        "def apply_reduce(data, func):\n",
        "    '''\n",
        "        Apply the function on a list of elements concatenating the samples with \n",
        "        the previous results (as the reduce).\n",
        "    '''\n",
        "    res = data.pop(0)\n",
        "    for el in data:\n",
        "        res = func((res, el))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "U1pTuqZ1FwWX",
      "metadata": {
        "id": "U1pTuqZ1FwWX"
      },
      "outputs": [],
      "source": [
        "def tokenize_samples(dataset: datasets.arrow_dataset.Dataset, tokenizer, max_embed_quest: int=512,\n",
        "                     max_embed_answ: int=30, mode: Optional[str]='np', with_history: bool=False, \n",
        "                     batch_size: int=1024)->tuple:\n",
        "    '''\n",
        "        This function returns a dictionary with the outputs of the tokenizer.\n",
        "        As input we refer to the concatenation of the quesions with the relative\n",
        "        contexts and we need both tokenization and attention mask. For the\n",
        "        answers we only need the tokenization.\n",
        "        Parameters:\n",
        "            - dataset: datasets.arrow_dataset.Dataset\n",
        "                The dataset from which the function takes the data.\n",
        "            - max_embed_quest: int\n",
        "                The maximum length for the sequence context + questions, in our \n",
        "                case is 512.\n",
        "            - max_embed_answ: int\n",
        "                The maximum length for the tokenized answers.\n",
        "            - mode: bool\n",
        "                Wheter the tokenizer could return the outputs as lists of python\n",
        "                integers, 'tf' tensors, 'np' arrays or 'pt' for pytorch tensors.\n",
        "            - with_history: bool\n",
        "                If True then we concatenate the question with the history instead \n",
        "                of the simple context.\n",
        "            - batch_size: int\n",
        "                The batch size to use in for computing the tokenization.\n",
        "        Returns:\n",
        "            - tuple\n",
        "                A tuple with the tokenized questions+contexts, their relative\n",
        "                attention masks and the tokenized answers.\n",
        "                If 'with_history' is True it also returns the new dataset\n",
        "                with the history, the passed dataset otherwise.\n",
        "    '''\n",
        "    if with_history:\n",
        "        pandas_dataset = dataset.to_pandas()\n",
        "        assert isinstance(pandas_dataset, pd.DataFrame)\n",
        "\n",
        "        new_dataset = Dataset.from_pandas(create_history(pandas_dataset))\n",
        "        context = new_dataset[\"history\"]\n",
        "    else:\n",
        "        new_dataset = dataset\n",
        "        context = dataset['story']\n",
        "\n",
        "    ids = []\n",
        "    masks = []\n",
        "    ys = []\n",
        "\n",
        "    num_steps = len(new_dataset) // batch_size\n",
        "    last_batch_size = len(new_dataset) - (batch_size*num_steps)\n",
        "\n",
        "    for i in range(num_steps+1):\n",
        "        start = batch_size * i\n",
        "        # If we are in the last batch and it's > 0\n",
        "        if i >= num_steps:\n",
        "            if last_batch_size > 0:\n",
        "                batch_size = last_batch_size\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # Take a batch of samples\n",
        "        batch_quest = new_dataset['questions'][start:start+batch_size]\n",
        "        batch_ctx = context[start:start+batch_size]\n",
        "        batch_answ = new_dataset['answers'][start:start+batch_size]\n",
        "        \n",
        "        # Tokenize the batch\n",
        "        x_tok_batch = tokenizer(\n",
        "            batch_quest,\n",
        "            batch_ctx,\n",
        "            max_length=max_embed_quest,\n",
        "            truncation=\"only_second\",\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=mode,\n",
        "            return_token_type_ids=False\n",
        "        )\n",
        "        y_tok_batch = tokenizer(\n",
        "            batch_answ,\n",
        "            max_length=max_embed_answ,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=mode,\n",
        "            return_token_type_ids=False\n",
        "        )\n",
        "        # Create 3 lists for the results\n",
        "        ids.append(x_tok_batch['input_ids'])\n",
        "        masks.append(x_tok_batch['attention_mask'])\n",
        "        ys.append(y_tok_batch['input_ids'])\n",
        "    \n",
        "    # Merge the results of the different batches concatenating them\n",
        "    X_tok = {'input_ids': apply_reduce(ids, np.concatenate), \n",
        "             'attention_mask': apply_reduce(masks, np.concatenate)}\n",
        "    y_tok = apply_reduce(ys, np.concatenate)\n",
        "    \n",
        "    return X_tok, y_tok, new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pGrX37BsSQaM",
      "metadata": {
        "id": "pGrX37BsSQaM"
      },
      "source": [
        "### EXPERIMENTAL TOKENIZATION AND BATCH\n",
        "\n",
        "I implemented an alternative version of the function in which I use the map built-in function to tokenize in batch the datasets. It's less efficient but the code is more readable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TaX6j6H2SZN_",
      "metadata": {
        "id": "TaX6j6H2SZN_"
      },
      "outputs": [],
      "source": [
        "def apply_tok(sample, tokenizer, ctx='story', max_embed=512, max_embed_answ=30):\n",
        "    '''\n",
        "        The function that will be applied to all the rows of the dataset.\n",
        "    '''\n",
        "    x_tok_batch = tokenizer(\n",
        "        sample['questions'],\n",
        "        sample[ctx],\n",
        "        max_length=max_embed,\n",
        "        truncation=\"only_second\",\n",
        "        padding=\"max_length\",\n",
        "        return_tensors='np',\n",
        "        return_token_type_ids=False\n",
        "    )\n",
        "    y_tok_batch = tokenizer(\n",
        "        sample['answers'],\n",
        "        max_length=max_embed_answ,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors='np',\n",
        "        return_token_type_ids=False\n",
        "    )\n",
        "    return {'input_ids':x_tok_batch['input_ids'],'attention_mask':x_tok_batch['attention_mask'],\n",
        "            'tok_answers':y_tok_batch['input_ids']}\n",
        "\n",
        "\n",
        "def experimental_tok(dataset: datasets.arrow_dataset.Dataset, tokenizer, max_embed_quest: int=512,\n",
        "            max_embed_answ: int=30, mode: Optional[str]='np', with_history: bool=False, \n",
        "            batch_size: int=1024):\n",
        "    '''\n",
        "        An alternative tokenization function.\n",
        "    '''\n",
        "    \n",
        "    if with_history:\n",
        "        pandas_dataset = dataset.to_pandas()\n",
        "        assert isinstance(pandas_dataset, pd.DataFrame)\n",
        "\n",
        "        new_dataset = Dataset.from_pandas(create_history(pandas_dataset))\n",
        "        context = \"history\"\n",
        "    else:\n",
        "        new_dataset = dataset\n",
        "        context = 'story'\n",
        "\n",
        "    tok_ds = new_dataset.map(apply_tok, batched=True, batch_size=batch_size, \n",
        "                        fn_kwargs={'tokenizer': tokenizer, 'ctx': context, \n",
        "                                   'max_embed':max_embed_quest, \n",
        "                                   'max_embed_answ': max_embed_answ}\n",
        "                             )\n",
        "    x_tok = {'input_ids': np.array(tok_ds['input_ids']), \n",
        "             'attention_mask': np.array(tok_ds['attention_mask'])}\n",
        "    y_tok = np.array(tok_ds['tok_answers'])\n",
        "\n",
        "    return x_tok, y_tok, new_dataset\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8EXn3NrYo6D",
      "metadata": {
        "id": "b8EXn3NrYo6D"
      },
      "source": [
        "### Tokenize data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BaahRWYRyeML",
      "metadata": {
        "id": "BaahRWYRyeML"
      },
      "source": [
        "Actually before tokenization occurs we create the Dataset objects (Huggingface) from the DataFrames. Then the tokenization function returns as result a dictionary of numpy array ('input_ids' and 'attention_mask') that is used as input for the model and a numpy array for the tokenized answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "CSDEDS1XzjXQ",
      "metadata": {
        "id": "CSDEDS1XzjXQ"
      },
      "outputs": [],
      "source": [
        "dataset_train = Dataset.from_pandas(df_train)\n",
        "dataset_val = Dataset.from_pandas(df_val)\n",
        "dataset_test = Dataset.from_pandas(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60g6Nu2z8QE",
      "metadata": {
        "id": "f60g6Nu2z8QE"
      },
      "source": [
        "#### Load pre-saved tokenized data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vso3dvQv2aBE",
      "metadata": {
        "id": "Vso3dvQv2aBE"
      },
      "source": [
        "If you prefer to load the tokenized data from .json files. \n",
        "\n",
        "The .json files, one for training, validation and test, contain:\n",
        "- 'input_ids': the tokenization of the concatenation of the answer and the context (+ the history if selected);\n",
        "- 'attention_mask': the attention masks for the input_ids;\n",
        "- 'labels': the target arrays that represent the tokenized answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z2St3eHn0FT1",
      "metadata": {
        "id": "z2St3eHn0FT1"
      },
      "outputs": [],
      "source": [
        "def load_tokenized_dict(file_path: str) -> Union[Tuple[Dict, np.ndarray], Tuple[None, None]]:\n",
        "    '''\n",
        "        It load the .json file from the given path, it convert the lists to numpy\n",
        "        arrays and at the end it returns a dictionary with 'input_ids' and \n",
        "        'attention_mask' and the array of the tokenized answers.\n",
        "        Parameters:\n",
        "            - file_path: str\n",
        "                The path of the .json file from which we read the data.\n",
        "            \n",
        "        Returns:\n",
        "            - Tuple[Dict[np.ndarray, np.ndarray], np.ndarray] | Tuple[None, None]\n",
        "                A tuple, the dictionary with the 'input_ids' and the 'attention_mask'\n",
        "                numpy arrays if the path exists, None otherwise.\n",
        "    '''\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r') as fin:\n",
        "            data = json.load(fin)\n",
        "        # It takes the labels and delete them from the dict\n",
        "        y_labels = data['labels']\n",
        "        del data['labels']\n",
        "\n",
        "        data['input_ids'] = np.array(data['input_ids'])\n",
        "        data['attention_mask'] = np.array(data['attention_mask'])\n",
        "\n",
        "        return data, np.array(y_labels) \n",
        "    else:\n",
        "        print(f\"ERROR: a file doesn't exist in the specified path: {file_path}.\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yjSYd91n0Hou",
      "metadata": {
        "id": "yjSYd91n0Hou"
      },
      "outputs": [],
      "source": [
        "#  Import the tokenized data from the json files\n",
        "load_dir = 'history'\n",
        "is_hist = '_hist' if load_dir=='history' else ''\n",
        "\n",
        "x_train, y_train = load_tokenized_dict(os.path.join(\n",
        "                    drive_tokenized_path, load_dir, f'train_tok{is_hist}.json'))\n",
        "\n",
        "x_val, y_val = load_tokenized_dict(os.path.join(\n",
        "                    drive_tokenized_path, load_dir, f'val_tok{is_hist}.json'))\n",
        "\n",
        "x_test, y_test = load_tokenized_dict(os.path.join(\n",
        "                    drive_tokenized_path, load_dir, f'test_tok{is_hist}.json'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q7FiFQD0Yv-n",
      "metadata": {
        "id": "Q7FiFQD0Yv-n"
      },
      "source": [
        "#### Compute tokenization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6fJhsbxH04DJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fJhsbxH04DJ",
        "outputId": "a3e6a279-d867-4c0c-cc65-43bac50eb044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "START: Creating the history column for the dataset...\n",
            "END: The column for the history has been added to the dataset.\n",
            "START: Creating the history column for the dataset...\n",
            "END: The column for the history has been added to the dataset.\n",
            "START: Creating the history column for the dataset...\n",
            "END: The column for the history has been added to the dataset.\n"
          ]
        }
      ],
      "source": [
        "# Compute the tokenized data, if you specify a different dimension, only on a subset\n",
        "# of the dataset.\n",
        "train_samples = len(dataset_train)\n",
        "val_samples = len(dataset_val)\n",
        "history = True\n",
        "\n",
        "x_train, y_train, hist_train_dataset = tokenize_samples(\n",
        "    dataset_train.select(range(train_samples)),\n",
        "    tokenizer, with_history=history, batch_size=16384\n",
        "    )\n",
        "\n",
        "x_val, y_val, hist_val_dataset = tokenize_samples(\n",
        "    dataset_val.select(range(val_samples)),\n",
        "    tokenizer, with_history=history, batch_size=4096\n",
        "    )\n",
        "\n",
        "x_test, y_test, hist_test_dataset = tokenize_samples(\n",
        "    dataset_test, tokenizer, with_history=history, batch_size=1024\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5KDoYgi13amI",
      "metadata": {
        "id": "5KDoYgi13amI"
      },
      "source": [
        "##### Save tokenized data in .json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7QxfGi7HnwkD",
      "metadata": {
        "id": "7QxfGi7HnwkD"
      },
      "outputs": [],
      "source": [
        "def save_tokenized_dict(x_data: dict, y_data: np.ndarray, file_path: str='./tokenized.json'):\n",
        "    '''\n",
        "        It saves the given data, a dictionary with 'input_ids' and \n",
        "        'attention_mask', as a .json file in the given_path. The values in the\n",
        "        x_data needs to be numpy arrays otherwise an error is raised due to the\n",
        "        'tolist()' function. \n",
        "        Parameters:\n",
        "            - x_data: dict\n",
        "                The dictionary that contains 'input_ids' and 'attention_mask'.\n",
        "            - y_data: np.ndarray\n",
        "                The array of tokenized answers.\n",
        "            - file_path: str\n",
        "                The path where the file will be saved.\n",
        "    '''\n",
        "    x_data['input_ids'] = x_data['input_ids'].tolist()\n",
        "    x_data['attention_mask'] = x_data['attention_mask'].tolist()\n",
        "    # Add the labels to compress all in a unique json\n",
        "    x_data['labels'] = y_data.tolist()\n",
        "\n",
        "    with open(file_path, 'w+') as fout:\n",
        "        json.dump(x_data, fout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cL1B2CsTnyKF",
      "metadata": {
        "id": "cL1B2CsTnyKF"
      },
      "outputs": [],
      "source": [
        "save_to = 'history'\n",
        "is_history_save = '_hist' if save_to=='history' else ''\n",
        "\n",
        "save_tokenized_dict(\n",
        "    x_train.data, y_train, \n",
        "    os.path.join(drive_tokenized_path, save_to, f'train_tok{is_history_save}.json')\n",
        "    )\n",
        "\n",
        "save_tokenized_dict(\n",
        "    x_val.data, y_val,\n",
        "    os.path.join(drive_tokenized_path, save_to, f'val_tok{is_history_save}.json')\n",
        "    )\n",
        "\n",
        "save_tokenized_dict(\n",
        "    x_test.data, y_test, \n",
        "    os.path.join(drive_tokenized_path, save_to, f'test_tok{is_history_save}.json')\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zP9VsaqkfIhm",
      "metadata": {
        "id": "zP9VsaqkfIhm"
      },
      "source": [
        "### Train the model [TASK 6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71xy7mv_fRKb",
      "metadata": {
        "id": "71xy7mv_fRKb"
      },
      "outputs": [],
      "source": [
        "def train_model(model: TFEncoderDecoderModel, X_train: dict, y_train: np.ndarray, epochs: int=3,\n",
        "                batch_size: int=32, optimizer: keras.optimizers.Optimizer=Adam(1e-5),\n",
        "                X_val: Optional[dict]=None, y_val: Optional[dict]=None) -> keras.callbacks.History:\n",
        "    '''\n",
        "        It trains the given model for the specified number of epochs, the given\n",
        "        optimizer and the given batch. It returns the history\n",
        "        Parameters:\n",
        "            - model: TFEncoderDecoderModel\n",
        "                The encoder decoder model to train on the data.\n",
        "            - X_train: dict\n",
        "                The dictionary that contains 'input_ids' and 'attention_mask'\n",
        "                arrays.\n",
        "            - y_train: np.ndarray\n",
        "                The array of the tokenized answers.\n",
        "            - epochs: int\n",
        "                The number of epochs to train the model for.\n",
        "            - batch_size: int\n",
        "                The batch size to use during the training.\n",
        "            - optimizer: keras.optimizers.Optimizer\n",
        "                The optimizer to consider for the training.\n",
        "            - X_val: dict | None\n",
        "                The X data for the validation dataset.\n",
        "            - y_val: dict | None\n",
        "                The y data for the validation dataset.\n",
        "        \n",
        "        Returns:\n",
        "            - keras.callbacks.History\n",
        "                The history of the training.\n",
        "    '''\n",
        "    validation_data = None if (X_val is None or y_val is None) else (X_val, y_val) \n",
        "\n",
        "    model.compile(optimizer=optim)\n",
        "\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
        "                        validation_data=validation_data)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dgtYeRav5DdT",
      "metadata": {
        "id": "dgtYeRav5DdT"
      },
      "source": [
        "We only need to fine-tune the model on our data so we keep a low learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xTKA9ZKPKd22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTKA9ZKPKd22",
        "outputId": "65051afa-5f75-441f-999f-ff572f1de09a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:371: FutureWarning: Version v4.17.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.17.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  return py_builtins.overload_of(f)(*args)\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_encoder_decoder_model/encoder/bert/pooler/dense/kernel:0', 'tf_encoder_decoder_model/encoder/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_encoder_decoder_model/encoder/bert/pooler/dense/kernel:0', 'tf_encoder_decoder_model/encoder/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2607/2607 [==============================] - 506s 189ms/step - loss: 0.9669 - val_loss: 0.7058\n",
            "Epoch 2/3\n",
            "2607/2607 [==============================] - 491s 188ms/step - loss: 0.6928 - val_loss: 0.6466\n",
            "Epoch 3/3\n",
            "2607/2607 [==============================] - 491s 188ms/step - loss: 0.6382 - val_loss: 0.6255\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "epochs = 3\n",
        "lr=3e-5\n",
        "\n",
        "optim = Adam(lr)\n",
        "\n",
        "# Set the desired seed for reproducibility\n",
        "set_reproducibility(seeds[0])\n",
        "history = train_model(model, x_train, y_train, epochs, batch_size, optim, \n",
        "                      x_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2Qaj_AVOwhzo",
      "metadata": {
        "id": "2Qaj_AVOwhzo"
      },
      "outputs": [],
      "source": [
        "#model.save_pretrained(os.path.join(drive_weights_path, 'bert-tiny', 'full_no_history'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s9RkOY87T9_n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "s9RkOY87T9_n",
        "outputId": "e8f3f053-8604-4e85-b6bd-da76d3f9bee7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_loss(history, min_y, max_y):\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylim(min_y, max_y)\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(history, 0.5, 0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F0n3q0oAWhBE",
      "metadata": {
        "id": "F0n3q0oAWhBE"
      },
      "source": [
        "### Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qosaLflzfMyQ",
      "metadata": {
        "id": "qosaLflzfMyQ"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate -q\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jjg-8DnZ5Taw",
      "metadata": {
        "id": "Jjg-8DnZ5Taw"
      },
      "source": [
        "We implemented one function to see the answers that the model gives and another one for measuring the f1 score (SQUAD). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "AxSPkpPuUeOo",
      "metadata": {
        "id": "AxSPkpPuUeOo"
      },
      "outputs": [],
      "source": [
        "def test_model(model: TFEncoderDecoderModel, dataset: datasets.arrow_dataset.Dataset, \n",
        "               max_length:int=20, idx_sample: Union[int, List[int]]=88, with_history=False) -> None:\n",
        "    '''\n",
        "        This function takes as input a dataset, it runs inference on the given\n",
        "        model and then it prints the question, the real answer and the predicted\n",
        "        one.\n",
        "        Parameters:\n",
        "            - model: transformers.models\n",
        "                The model to run inference on.\n",
        "            - dataset: datasets.arrow_dataset.Dataset\n",
        "                The dataset from which the function takes the samples.\n",
        "            - max_length: int\n",
        "                It's the maximum length allowed during the generation.\n",
        "            - idx_sample: int | list[int]\n",
        "                If an integer is passed, then the function computes the answer\n",
        "                for a single sample, otherwise for each sample specified in the\n",
        "                list (through the index).\n",
        "            - with_history: bool\n",
        "                if to tokenize with the history or not.\n",
        "\n",
        "        Return:\n",
        "            The function simply prints the results without returning anything.\n",
        "    '''\n",
        "    if isinstance(idx_sample, int):\n",
        "        idx_sample = [idx_sample]\n",
        "\n",
        "    # Check if indices are without dataset bounds    \n",
        "    max_ind = max(idx_sample)\n",
        "    if max_ind >= len(dataset):\n",
        "        print(f\"Error: index {max_ind} is out of bounds (dataset of length {len(dataset)}.\")\n",
        "        return None\n",
        "\n",
        "    for idx in idx_sample:\n",
        "        inp_test, _, _ = tokenize_samples(dataset.select([idx]), tokenizer, with_history=with_history)\n",
        "\n",
        "        greedy_output = model.generate(inp_test['input_ids'],\n",
        "                                       attention_mask=inp_test['attention_mask'],\n",
        "                                       max_length=max_length)# decoder_start_token_id=model.config.decoder.bos_token_id\n",
        "\n",
        "        print(f\"- The question {idx+1} is:\\n\\t{dataset[idx]['questions']}\")\n",
        "        print(f\"- The actual answer is:\\n\\t{dataset[idx]['answers']}\")\n",
        "        print(f\"- The predicted answer is:\\n\\t\"\n",
        "              f\"{tokenizer.decode(greedy_output.numpy()[0], skip_special_tokens=True)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VZ5R5Hwza23p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ5R5Hwza23p",
        "outputId": "4bdc1fd0-2759-4ce3-aa8d-49240ef89c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- The question 89 is:\n",
            "\tWas there a storm headed that way?\n",
            "- The actual answer is:\n",
            "\tYes\n",
            "- The predicted answer is:\n",
            "\tno\n",
            "\n",
            "- The question 1 is:\n",
            "\tWhat color was Cotton?\n",
            "- The actual answer is:\n",
            "\twhite\n",
            "- The predicted answer is:\n",
            "\ta dog\n",
            "\n",
            "- The question 1189 is:\n",
            "\tIs Lily old?\n",
            "- The actual answer is:\n",
            "\tno\n",
            "- The predicted answer is:\n",
            "\tyes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_model(model, dataset_test, max_length=50, idx_sample=[88, 0, 1188])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-GZeLV4nWbtD",
      "metadata": {
        "id": "-GZeLV4nWbtD"
      },
      "source": [
        "### Evaluate F1 score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vlBMZ2sq60co",
      "metadata": {
        "id": "vlBMZ2sq60co"
      },
      "source": [
        "\n",
        "In order to compute the squad f1 score we decided to use the [squad metric](https://huggingface.co/spaces/evaluate-metric/squad) from Huggingface. \n",
        "\n",
        "To avoid problem with RAM allocation (model.generate gave OOM error if we tried to run on the full test set), we generate batch of results and then we merge them together before computing the f1 score. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "PzhezMkW6v_Y",
      "metadata": {
        "id": "PzhezMkW6v_Y"
      },
      "outputs": [],
      "source": [
        "def compute_f1_score(model: TFEncoderDecoderModel, X: dict, answers: List[str],\n",
        "                     metric: evaluate.EvaluationModule, num_samples: Optional[int]=None,\n",
        "                     max_length: int=20, batch_size: int=500, \n",
        "                     return_pred_answers: bool=False) -> Tuple[float, list]:\n",
        "    '''\n",
        "        This function takes as input a dataset, it runs inference on the given\n",
        "        model and then it returns the F1 score metric.\n",
        "        Parameters:\n",
        "            - model: transformers.models\n",
        "                The model to run inference on.\n",
        "            - X: dict\n",
        "                The structure that contains the input_ids and attention_mask, it\n",
        "                is obtained doing 'variable.data' on an object of type\n",
        "                transformers.tokenization_utils_base.BatchEncoding.\n",
        "            - answers: list\n",
        "                The array of strings of answers.\n",
        "            - metric: evaluate.Metric\n",
        "                It's the metric object to consider for the computation.\n",
        "            - num_samples: int \n",
        "                The function takes from the samples the first 'num_samples'\n",
        "                samples.\n",
        "            - max_length: int\n",
        "                It's the maximum length used during the generation\n",
        "            - batch_size: int\n",
        "                It's the batch size used to pass this number of answers to the \n",
        "                model (to avoid RAM allocation error).\n",
        "            - return_pred_answers: bool\n",
        "                If the function has to return the array of predicted answers.\n",
        "\n",
        "        Returns:\n",
        "            - (float, list)\n",
        "                The f1 score and the array of predicted answers if return_pred_answers\n",
        "                is True, only the f1 score and an empty list otherwise.\n",
        "    '''\n",
        "    # Take all the samples\n",
        "    if num_samples == None:\n",
        "        num_samples = len(answers)\n",
        "    \n",
        "    assert num_samples > batch_size, \"ERROR: the batch size is greater than the number of samples.\"\n",
        "    \n",
        "    ids, att_mask = X['input_ids'][:num_samples], X['attention_mask'][:num_samples]\n",
        "\n",
        "    num_step = num_samples//batch_size\n",
        "    # The last batch may be smaller than the batch_size \n",
        "    last_batch_size = num_samples - (num_step*batch_size)\n",
        "\n",
        "    predicted_answers = []\n",
        "    for i in range(num_step+1):\n",
        "        start = batch_size*(i)\n",
        "\n",
        "        # Check if it has to run the last batch with a different size\n",
        "        if i >= num_step:\n",
        "            if last_batch_size > 0:\n",
        "                batch_size = last_batch_size\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        out = model.generate(ids[start:start+batch_size], \n",
        "                             attention_mask=att_mask[start:start+batch_size],\n",
        "                             max_length=max_length).numpy()\n",
        "\n",
        "        refs = []\n",
        "        preds = []\n",
        "        for j in range(start, start+batch_size):\n",
        "            # The model output is always of 'batch_size' elements so I need to\n",
        "            # access the j - start element\n",
        "            pred_string = tokenizer.decode(out[j-start], skip_special_tokens=True)\n",
        "            if return_pred_answers:\n",
        "                predicted_answers.append(pred_string)\n",
        "\n",
        "            preds.append({'prediction_text':pred_string, 'id':str(j)})\n",
        "            refs.append({'answers': {'text':[answers[j]],'answer_start':[0],},'id':str(j)})\n",
        "\n",
        "        metric.add_batch(predictions=preds, references=refs)\n",
        "\n",
        "    f1_score = metric.compute()\n",
        "    assert f1_score is not None\n",
        "    \n",
        "    return f1_score['f1'], predicted_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hlILO44SU2Sz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlILO44SU2Sz",
        "outputId": "d7fa38d6-273d-4273-9a72-2c77336fe105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The f1 score is 13.405680520723562\n"
          ]
        }
      ],
      "source": [
        "f1_metric = evaluate.load(\"squad\")\n",
        "\n",
        "f1_score, pred_answers = compute_f1_score(model, x_test, dataset_test['answers'],\n",
        "                            f1_metric, num_samples=None, max_length=20, \n",
        "                            batch_size=1000, return_pred_answers=True)\n",
        "\n",
        "print(f\"The f1 score is {f1_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AntOxXv8C44o",
      "metadata": {
        "id": "AntOxXv8C44o"
      },
      "source": [
        "#### Compute top-5 better and worst questions\n",
        "In the following function I compute the 5 best and 5 worst answers for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "ogU00_ZAzI0F",
      "metadata": {
        "id": "ogU00_ZAzI0F"
      },
      "outputs": [],
      "source": [
        "# Group dialogues by source and report the worst 5 model errors for each source (w.r.t. SQUAD F1-score).\n",
        "def decode_answ(sample, tokenizer):\n",
        "    '''\n",
        "        It decodes an array of numbers using the passed tokenizer.\n",
        "    '''\n",
        "    return tokenizer.decode(sample, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def compute_f1_single_questions(predicted_answers: List[str], answers: List[str],\n",
        "                                metric) -> List[int]:\n",
        "    '''\n",
        "        It computes the f1 score for each single question and returns the scores.\n",
        "        Parameters:\n",
        "            - predicted_answers: list\n",
        "                The list of predicted answers (strings), that you can get from the \n",
        "                'compute_f1_score' function.\n",
        "            - answers: list\n",
        "                The list of actual answers.\n",
        "            - metric:\n",
        "                In this case a f1 squad metric is needed because we add the batches\n",
        "                with a predefined schema.\n",
        "\n",
        "        Returns:\n",
        "            - list\n",
        "                The list of f1 scores for each sample.\n",
        "    '''\n",
        "    scores = []\n",
        "\n",
        "    # Compute the f1 score for each sample\n",
        "    for pred, real in zip(predicted_answers, answers):\n",
        "        pr={'prediction_text':pred, 'id':str(0)}\n",
        "        re={'answers': {'text':[real],'answer_start':[0],},'id':str(0)}\n",
        "\n",
        "        scores.append(metric.compute(predictions=[pr], references=[re])['f1'])\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "eQ-tTXejEgBd",
      "metadata": {
        "id": "eQ-tTXejEgBd"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "def compute_top_worst_k(scores, questions, answers, predictions, k=5):\n",
        "    '''\n",
        "        It computes the top and worst k questions (considering f1 score).\n",
        "    '''\n",
        "    idx_sort = np.argsort(scores)\n",
        "    # We revert the list because the best score is the last one (highest)\n",
        "    top_k = idx_sort[-k:][::-1]\n",
        "    worst_k = idx_sort[:k]\n",
        "\n",
        "    data_top = [(scores[i], questions[i], answers[i], predictions[i]) for i in top_k]\n",
        "    data_worst = [(scores[i], questions[i], answers[i], predictions[i]) for i in worst_k]\n",
        "\n",
        "    ranking_top = pd.DataFrame(data_top, columns=[\"F1 SCORE\",\"Question\",\"Answer\",\"Predicted\"])\n",
        "    ranking_worst = pd.DataFrame(data_worst, columns=[\"F1 SCORE\",\"Question\",\"Answer\",\"Predicted\"])\n",
        "    print(f\"The top five f1 scores are:\")\n",
        "    display(ranking_top)\n",
        "    print(f\"\\nThe worst five f1 scores are:\")\n",
        "    display(ranking_worst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fL7znhif_Xro",
      "metadata": {
        "id": "fL7znhif_Xro"
      },
      "outputs": [],
      "source": [
        "f1_scores_quest, preds, answers = compute_f1_single_questions(model, tokenizer, x_test,\n",
        "                                                        dataset_test['answers'],\n",
        "                                                        f1_metric)\n",
        "\n",
        "compute_top_worst_k(f1_scores_quest, df_final_evaluation['questions'], answers, preds, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZWcxygHiJOdI",
      "metadata": {
        "id": "ZWcxygHiJOdI"
      },
      "source": [
        "#### Compute f1 by source and dialogue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "id": "21lJXahvca8q",
      "metadata": {
        "id": "21lJXahvca8q"
      },
      "outputs": [],
      "source": [
        "# TODO: maybe it's faster if we compute all the pred and ref and then\n",
        "# we add_batch to the metric\n",
        "def compute_f1_group(sample, metric):\n",
        "    '''\n",
        "        It computes the f1 score and the sum of the questions, answers and predicted\n",
        "        answers for the samples within a same dialogue provided by a source.\n",
        "    '''\n",
        "    sumstr = []\n",
        "    for i, (ans, pr) in enumerate(zip(sample['answers'], sample['predictions'])):\n",
        "        pred = {'prediction_text':pr, 'id':str(i)}\n",
        "        ref = {'answers': {'text':[ans],'answer_start':[0]},'id':str(i)}\n",
        "        metric.add(prediction=pred, reference=ref)\n",
        "        sumstr.append(f\"Q:{sample['questions'].iloc[i]} A:{ans} P:{pr}\")\n",
        "\n",
        "    return pd.Series({'f1':metric.compute()['f1'], 'dialogue': sumstr})\n",
        "\n",
        "\n",
        "def f1_score_source_dialogue(df, predictions, metric, drop_index=False):\n",
        "    '''\n",
        "        Returns a DataFrame with 2 additional columns, one for the f1 score at\n",
        "        a dialogue level and another for the dialogue with the questions, the \n",
        "        answers and the predicted answers.\n",
        "    '''\n",
        "    assert len(predictions) == len(df), \"ERROR: predictions array has not the same length of the DataFrame\"\n",
        "    \n",
        "    new_df = df.copy()\n",
        "    new_df['predictions'] = predictions\n",
        "\n",
        "    new_df = new_df.groupby(['source','story'], sort=True) \\\n",
        "                    .apply(compute_f1_group, metric=metric) \\\n",
        "                    .reset_index(drop=drop_index)\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dxEsDFoXgnta",
      "metadata": {
        "id": "dxEsDFoXgnta"
      },
      "outputs": [],
      "source": [
        "df_f1 = f1_score_source_dialogue(df_final_evaluation, pred_answers, f1_metric)\n",
        "df_f1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1814004",
      "metadata": {
        "id": "f1814004"
      },
      "source": [
        "# Assignment Evaluation\n",
        "\n",
        "The following assignment points will be awarded for each task as follows:\n",
        "\n",
        "* Task 1, Pre-processing $\\rightarrow$ 0.5 points.\n",
        "* Task 2, Dataset Splitting $\\rightarrow$ 0.5 points.\n",
        "* Task 3 and 4, Models Definition $\\rightarrow$ 1.0 points.\n",
        "* Task 5 and 6, Models Training and Evaluation $\\rightarrow$ 2.0 points.\n",
        "* Task 7, Analysis $\\rightarrow$ 1.0 points.\n",
        "* Report $\\rightarrow$ 1.0 points.\n",
        "\n",
        "**Total** = 6 points <br>\n",
        "\n",
        "We may award an additional 0.5 points for outstanding submissions. \n",
        " \n",
        "**Speed Bonus** = 0.5 extra points <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20a1b2b9",
      "metadata": {
        "id": "20a1b2b9"
      },
      "source": [
        "## Report\n",
        "\n",
        "We apply the rules described in Assignment 1 regarding the report.\n",
        "* Write a clear and concise report following the given overleaf template (**max 2 pages**).\n",
        "* Report validation and test results in a table.$^1$\n",
        "* **Avoid reporting** code snippets or copy-paste terminal outputs $\\rightarrow$ **Provide a clean schema** of what you want to show"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0967c209",
      "metadata": {
        "id": "0967c209"
      },
      "source": [
        "## Comments and Organization\n",
        "\n",
        "Remember to properly comment your code (it is not necessary to comment each single line) and don't forget to describe your work!\n",
        "\n",
        "Structure your code for readability and maintenance. If you work with Colab, use sections. \n",
        "\n",
        "This allows you to build clean and modular code, as well as easy to read and to debug (notebooks can be quite tricky time to time)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23929660",
      "metadata": {
        "id": "23929660"
      },
      "source": [
        "## FAQ (READ THIS!)\n",
        "\n",
        "---\n",
        "\n",
        "**Question**: Does Task 3 also include data tokenization and conversion step?\n",
        "\n",
        "**Answer:** Yes! These steps are usually straightforward since ```transformers``` also offers a specific tokenizer for each model.\n",
        "\n",
        "**Example**: \n",
        "\n",
        "```\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "encoded_text = tokenizer(text)\n",
        "%% Alternatively\n",
        "inputs = tokenizer.tokenize(text, add_special_tokens=True, max_length=min(max_length, 512))\n",
        "input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
        "```\n",
        "\n",
        "**Suggestion**: Hugginface's documentation is full of tutorials and user-friendly APIs.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "**Question**: I'm hitting **out of memory error** when training my models, do you have any suggestions?\n",
        "\n",
        "**Answer**: Here are some common workarounds:\n",
        "\n",
        "1. Try decreasing the mini-batch size\n",
        "2. Try applying a different padding strategy (if you are applying padding): e.g. use quantiles instead of maximum sequence length\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c56a612",
      "metadata": {
        "id": "9c56a612"
      },
      "source": [
        "## Contact\n",
        "\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54bac4b9",
      "metadata": {
        "id": "54bac4b9"
      },
      "source": [
        "## The End!\n",
        "\n",
        "Questions?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "collapsed_sections": [
        "dL4rB4Dv4q5D",
        "11ada8c8",
        "66cfee64",
        "7b532042",
        "f6643e14",
        "bddb1b09",
        "icicV7K0sEzm",
        "C4RB6BoVsmgO",
        "pGrX37BsSQaM",
        "b8EXn3NrYo6D",
        "f60g6Nu2z8QE",
        "Q7FiFQD0Yv-n",
        "5KDoYgi13amI",
        "zP9VsaqkfIhm",
        "F0n3q0oAWhBE",
        "AntOxXv8C44o",
        "f1814004"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('tf_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3ce8273a8c38ff7795926bf915cbd2931d7313de696d3392f6c42417e8c00d4d"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09d2d44282ab4393a1194b3b2e22d0aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e68baefb0ea44be891c8dc781dc8ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5c72d8bac0146b78146706d434af159",
              "IPY_MODEL_83abbfc0af0e48f299320794de8efd6e",
              "IPY_MODEL_6d127a372be54e81af2b963bba3f780b"
            ],
            "layout": "IPY_MODEL_15a7fa9330594048b97a81c3c0d8d03f"
          }
        },
        "15a7fa9330594048b97a81c3c0d8d03f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f3ec4fc435f4a58a4073291bed6aaca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "280ac3d6edd541ac9a10af585918baf1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "287374fc9a99468b9fcf5e84792c29de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fb6e7edd7b74a918e051e7a0b8150a5",
            "max": 285,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7bc0fbac0f94a1a802a6270c9428c53",
            "value": 285
          }
        },
        "2c08a3707b3148858d407122ab78ee6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4273cbab512b4e1cab8f8d671b0c8982": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4830313952d04817afcc32ee388d3a2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "496f22c495e641d39b6c1de47a92ccf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fb6e7edd7b74a918e051e7a0b8150a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "571a2f1dcc1449058216e47ec26ae1a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57397b6e3f204e15bc7b51a646a04eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73f009ed1d2e48babeb14d86ee63f4a0",
              "IPY_MODEL_d0914d33146646ee8405229cb920f25e",
              "IPY_MODEL_c8c17855f3194af79a2cb93dee29d940"
            ],
            "layout": "IPY_MODEL_cae7cd66d398453c980725c505de8b77"
          }
        },
        "6d127a372be54e81af2b963bba3f780b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4830313952d04817afcc32ee388d3a2b",
            "placeholder": "​",
            "style": "IPY_MODEL_2c08a3707b3148858d407122ab78ee6c",
            "value": " 232k/232k [00:00&lt;00:00, 584kB/s]"
          }
        },
        "6e08563dbfca4c798e5a717c29664d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73f009ed1d2e48babeb14d86ee63f4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f3ec4fc435f4a58a4073291bed6aaca",
            "placeholder": "​",
            "style": "IPY_MODEL_dea4cc34b8af4b74946da392a5fc0d39",
            "value": "Downloading: 100%"
          }
        },
        "7b4eadf9922a40c3830fcb81b5cc359a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "808fe8190ddb46758922fe04b16ced4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83abbfc0af0e48f299320794de8efd6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_571a2f1dcc1449058216e47ec26ae1a0",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8ee2155725e4d71b19fa34e3c2dd81c",
            "value": 231508
          }
        },
        "9bfa7f13133e4968aca726f32c89ef1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c72d8bac0146b78146706d434af159": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7ec7aaa3490495aab850ae847c66027",
            "placeholder": "​",
            "style": "IPY_MODEL_4273cbab512b4e1cab8f8d671b0c8982",
            "value": "Downloading: 100%"
          }
        },
        "a7bc0fbac0f94a1a802a6270c9428c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5ec124b283c40078b611975b26754d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8ddbfb4953c4915915629b4772bc6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be3cadb57c6c4d908d611ace714fa9ac",
              "IPY_MODEL_287374fc9a99468b9fcf5e84792c29de",
              "IPY_MODEL_eb637e6eb0334cceb8cc12fa2c99cd37"
            ],
            "layout": "IPY_MODEL_808fe8190ddb46758922fe04b16ced4e"
          }
        },
        "be3cadb57c6c4d908d611ace714fa9ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_280ac3d6edd541ac9a10af585918baf1",
            "placeholder": "​",
            "style": "IPY_MODEL_6e08563dbfca4c798e5a717c29664d13",
            "value": "Downloading: 100%"
          }
        },
        "c8c17855f3194af79a2cb93dee29d940": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5ec124b283c40078b611975b26754d0",
            "placeholder": "​",
            "style": "IPY_MODEL_7b4eadf9922a40c3830fcb81b5cc359a",
            "value": " 17.8M/17.8M [00:00&lt;00:00, 27.0MB/s]"
          }
        },
        "cae7cd66d398453c980725c505de8b77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbff110e03de4dcab47f0dcd1144f442": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0914d33146646ee8405229cb920f25e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bfa7f13133e4968aca726f32c89ef1a",
            "max": 17756393,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbff110e03de4dcab47f0dcd1144f442",
            "value": 17756393
          }
        },
        "d8ee2155725e4d71b19fa34e3c2dd81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dea4cc34b8af4b74946da392a5fc0d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7ec7aaa3490495aab850ae847c66027": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb637e6eb0334cceb8cc12fa2c99cd37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09d2d44282ab4393a1194b3b2e22d0aa",
            "placeholder": "​",
            "style": "IPY_MODEL_496f22c495e641d39b6c1de47a92ccf7",
            "value": " 285/285 [00:00&lt;00:00, 1.98kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
